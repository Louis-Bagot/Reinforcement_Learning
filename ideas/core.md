# Core Ideas (goals) for Reinforcement Learning agents

## 1. Unsupervised Behavior Learning
One of the most fundamental ideas I want to get through to my agent is the ability to learn behaviors without extrinsic reward.
This is simply motivated by observation of human behavior (and a talk by LeCun): most of what we learn is Unsupervised. We figure out relationships, dynamics, and generalize over the environment, without any reward. Thanks to this, we can then act in the environment with varying goals - or even slightly varying dynamics -, efficiently. For example, in a motor task, we don't need to see the reward at the end to understand that we can move our arm around, reach out for objects, the possible interactions with them. This enables our policy search space to be much more consistent with the environment. Obviously, this only makes sense if the task requested *does* align logically within the environment dynamics (not a random string of actions with no particular meaning or whatnot), but this is actually a fair assumption to carve into the agent's "thought" process.
More formally? From reward-less interactions with the environment, the agent can learn fragment policies (as in, short-term action sequences; or HL controllers) to reach states or situations of *interest*, which can be defined in multiple ways. These include: states that are different from one another (again, defining the distance in many ways), or states in which our actions undoubtedly have an understandable result on the environment (certainty about the state-action dynamics). All in all, the agent can understand its impact on the environment. From this set of fragment policies (it sounds at *lot* like HL), the agent can then explore a more *consistent* set of meta-policies. Ideally, the agent would be able to understand their outcome to enable planning in meta-space, with a relational component to make the planning much more consistent again.
(I'm not too satisfied with this formalization.)
Since it seems so close, let's express this in terms of HL. I want the learning of *options* (`TOREAD` HL theory paper; was that the name given to fragment policies?) to be made automatic, independent of the extrinsic reward. This seems to call for Hindsight; but feels like it reaches beyond that.
Another formulation is: we don't want to directly try to solve some goal (reward-based policy search), but rather explore, through a bit of consistent *curiosity*, the state-action/policy space independently, and efficiently learn environment dynamics and policies from this experience.

How can we achieve this, then? (more practically)
### Hindsight
From what I understood, hindsight is the idea that you can learn the behaviors you just realized involuntarily. Learn that whatever sequence of moves you just did led to the outcome you reached; which means a bit learning a model of the environment, but also and beyond that, learning a policy that can achieve the observed behavior.

### Unsupervised Learning
The core idea is simply *generalization*. Situations, states, histories that are similar in ways, and therefore should result in similar outputs. The process by which we want to do this generalization is the interesting part: Unsupervised. Simply classify (or build a distance measure) various properties of our MDP to solve a much lower-level dimensional problem. Informally, we want to build some kind of understanding of the MDP (perhaps without reward), especially our impact on the dynamics, from experience.
Formulated differently, we want to rid ourselves of the environment goal; first developing (from curiosity or whatever) a useful set of policies, and *then* applying them to whatever task we're asked - the idea that if we just change the reward function (but not the whole MDP), much of our knowledge still applies.

* Can we learn some kind of **classification over policies**; linking policies to each other based on their outcome? This would enable to reduce the policy search space by having an idea of policy proximity and different ways to achieve a same goal. More clearly: there's tons of ways to move your arm around, nearly similarly, to achieve the same thing. Such high-dimensional problems, both from an input and output point of view, can often be dramatically be reduced to same *ideas* by generalization, and the understanding that the overall concept is the same. A whole neighborhood of fragment policies can achieve the same result; if Unsupervised Learning can hint on that with some kind of distance measure over policies, the data-efficiency should be dramatically improved.
* **Unsupervised classification of states** (or ideally, latent spaces): perform similar behaviors (atomic/primitive actions) in similar states. This could allow for planning in state-space more efficiently and enable uncertainty to be dealt with in a systematic procedure. This could also help curiosity (next section) by hinting that this state has 'pretty much' already been experienced.

### Artificial Curiosity
When the agent knows nothing of the environment, what should it do? Epsilon-greedy is the simplest (and most frustrating) idea, just randomly trying actions and hoping to get some extrinsic reward from it, to start hunting for it.
How can we beat it?
We want a more consistent approach to *exploration*.
The simplest idea is to strive for the **discovery of new, unseen states** (-action pairs). `TOREAD` all the papers about that \o/.

*Intuitive ideas:*
* motivate the search by **kicking the agent out of known territory** (negative reward for known states or dynamics). Should we record all the trajectories? Unsupervised Learning can help there, to generalize unseen states to similar ones from experience.
* Another idea is **surprise**: model-based. Motivate the search for new states by giving intrinsic reward for mistakes in the model.
* Curiosity in **policy space**: motivate new policies by giving reward for different policies, unseen trajectories. Once again Unsupervised Learning might be able to help there to classify what is different from what.

## 2. Model-based agents and planning
Why?
* The planning - or simply *predicting* (short-horizon planning?) ability of humans is hard to even quantify as it is so fundamental; we predict all the time. In our interactions with each other, or with the environment, we always have some kind of **expectation** of what is going to happen or what might happen (gestures, sentences; physics) which enables us to prepare for the different responses to the anticipated possible outcomes. This severely restrains the policy search space (where a policy can simply be a thought: e.g. anticipate the conclusion of a sentence or sequence thereof, so we can build up our understanding of it before finishing it -- "seeing where this is going" ?)
* Planning allows for much more efficient decision-making: simply **selecting the best action over a set of possible policies** and outcomes. This is a difference with just state-action function: when taking an action, we also consider the policy that fits with this choice (turning left means I'm probably not going to turn right back because my policy wanted me to go right in the first place): we compare policies in a state, not just actions with a single policy form thereon. (could be linked to strategical "plans" in chess, corresponding to completely different trajectories).
* Ideally, we would want to plan in **latent space** and **meta-action (option) space**. They are very linked. When I plan my day, I don't plan every single muscle move. I plan in a more abstract space: going to eat to this or this place, then going to see friends or exercise. We cna see latent space (lower dimensional information than the actual sensory input at the future time), and meta-actions ("going there" implies a very complex sequence of actions). There is a third element coming up there: **simplified state space**. "Going to see friends" implies in a place, at a time, with different possible sets of friends. The latent-space would be the simple idea of all of this at once: not exactly which bar, or where this bottle is; but just the essential information. Knowing which friend is there *is* important in latent space because latent space simply reduces dimensionality (the idea of the friend; not their daily haircut and clothes and everything). The idea of "simplified" state space is then to abstract the latent space once more to give general concepts - this could be seen as another bird's eye view on the same situation; simply smoothing out the space. There are multiple layers of detail for a single situation, which can be seen as abstractions. Instead of creating an entire different idea with "simplified state space", we can rather see it as degrees of confidence in the state approximation (granularity). The granularity isn't necessarily decreasing over time; when planning we might need general ideas about some things, but very precise instructions on particular parts of the future. Therefore it could be interesting to simply use some kind of granularity variable \g_t that gives an idea of how precise we want our model to be for this planned trajectory. It could be defined as a measure over latent states; where we "don't mind if we are \g_t or less far away from the goal state" (distribution over friends and bars we might end up with and in). This distance measure needs to be quite precise then, as to what is and is not important. \g_t could apply to different components of the latent space. This can maybe be linked to the idea of *attention*; not sure if the definition is close in any way: `TOREAD` paper Attention.

## Ideas to dig into
* Ability to **generalize to open problems**: formulation of sub-goals that can restrict the span of possible actions and behaviors?
* **Hint the mechanisms of the environment in the agent architecture** - the brain doesn't start from scratch all the time; we are wired to solve physics problems and reality-consistent situations and do poorly with absurd environments. How can we stick these into the agent's fundamental wirings to help it? examples: CNN takes into account properties of images; HyperNEAT (and forgot the name of the nets there) build onto the assumption that the sensory world is often symmetrical and uses it to evolve architectures accordingly. Relational nets, also; although this is `TOREAD`
* **Highly deterministic environments**: most real-world (especially physical) problems have a very strong deterministic component; only some states (or time steps) have a stochastic component to them, while most other transitions and rewards are very consistent and easily predicted. This means that building a model of the environment might not, in these cases, be such a hard problem; and the distributional model we can learn from it might not be that complex, only branching out to multiple possibilities in specific (or just, few) cases. In planning, we can then build trajectories with some measure of confidence that they might occur (be sampled). This observation doesn't seem to impact in any way the algorithms&methods that might stem from the usual formalization; but it hints that it might be a practical possibility, while most models are just sample-based due to the assumption that distributional models are too complex to be approximated.
* **Off Policy planning and learning**: they raise multiple problems of off-policy learning such as high variance or the simple fact that it doesn't really work well with function approximation. The main cause seems to be the very nature of the method; i.e. that the data observed isn't the one of the policy we want to learn, so the learning process is tedious. However we really want to be able to have off-policy methods; first because we might not have absolute control over the data input (noisy actions, or learning by observing an expert), but also because we might want to learn multiple policies at once - various strategies to tackle the same problem. This joins what was said in the planning part: you sometimes want to consider a completely different behavior, rather than the "one-step" foreseeing of the Q value function; you might want to compare complete policies. This calls either for some kind of control over the environment, where we would have to rewind back up to compare policies; or for off-policy methods where we still use the data from just one but learn multiple. To get the best out of both off and on-policy methods, we could consider learning off-policy in planning space: we learn a model thanks to the behavior policy, and using it, learn about the off-policy but *on-policy* within a "dreamt" simulation. The main difference with simple look-ahead planning to find the best move or policy is that we could actually *learn* the off-policy in our simulation, not just use the outcome to make a decision (MCTS way, where everything is discarded at the end. Even AlphaZero discards the explored experiences)
* **Learning more from an MCTS search** - as we just said AlphaZero discards explored branches of the tree, in the sense that learning only occurs on the main (true-environment) experience. In that sense, it isn't a "complete" model-based algorithm as it nearly doesn't use its perfect model to learn, only to plan. From an MCTS search, the only thing we learn is the resulting search policy \pi. Is there a way to *learn more* from that search? Ideally, in a Meta setting, we would be able to understand the general guidelines of "learning" (since we have a base policy and an improved policy) to apply this learning process to future steps or environments altogether. From a practical point of view, we could learn the \pi policies of all the explored nodes of the tree, but give a weaker weight (e.g. exponentially decreasing weight with depth) to the updates.
* Can you train **MuZero with a GA**? Since it was trained end-to-end. You would still need the network to output value functions, in order to guide MCTS, which can be a major disadvantage - but could also not be - even if the values do not correspond to actual value functions, it could be useful. The GA could even come up with its own rewarding process in planning to boost some behaviors..? Sounds fun.
* **Importance sampling** seems soooo **random & arbitrary**. Why just divide the policies? More legit seems to be to take the conditional probability to end up in this position with policy pi given that you got there with policy b...
* Isn't a bit stupid that the agent should **understand the reward** by itself? I mean, if we were to play Gridworld, we would know which is the terminal state; doesn't really make sense (for real world problems) to have an environment where the goal is unknown before we start. Would seem more legit to *explain* the reward (goal) - one naive way to do it would be to backup a few times from the transitions that provide reward, so the agent knows a bit how to behave if it is close to a rewardy transition... Obviously, this doesn't apply to the general case and implies control and prior knowledge over the environment; but on many problems the goal is actually pretty easily "explainable"; the way to get there is not...?
* I'm surprised **Eligibility traces** aren't used much more than they seem; such a powerful tool! `TOREAD` the True Online TD(lambda) paper.
* Basically what evolution did was **evolve different rewarding signals** for the brain. The brain that attributed reward/punishment signals to the stimulus that resulted in survival were selected. Also obviously, the brain architecture itself was evolved; but it's interesting to think that we could try to run a simulation of animals and only evolve their reward structures, with one-hot survival as the fitness function. So more formally, the population would be reward functions (eg weights of a parameterized function as DNA) and the policy would be the resulting behavior of an RL agent when trained with said reward function. Ideally, obviously, we would also evolve different agent architectures, algorithms, etc. so the whole brain & body process. The here proposed algorithm, clearly, doesn't solve the sparse reward setting as the agents would never see the reward in the first place, except randomly (there is no direction to follow), so the GA is powerless. But what if we add a *curiosity* component to our fitness? Like a motivation to behave differently, explore the state-space, or whatever measure... And have it reduced over the number of successful goal reached? Anyway as Matthias mentioned, GA+RL is exponentially data-inefficient. Still interesting to see the resulting behaviors though. But wouldn't the evolved reward function actually simply learn the value function? Then the agent could learn to just follow reward greedily .^.
* Learning **Auxiliary tasks and goals** sharing weights with the rest of the network (or more generally, same agent until some point). The auxiliary tasks can be prediction of something else (physics, distribution, whatever other than the goal value function or policy); this will allow the network to have a wider generalization span over possible behaviors, as introduced in the book. they say it usually leads to faster training. The main question is: *what* auxiliary tasks, *how* to train them, and *why* (higher level question to understand what led to results)? For example, we might simply want to use some kind of future-predicting NN head with a shared body of the rest of the network; the weights learnt to do that could help understanding the environment on a more fundamental level and help the policy or value evaluation. (broadly speaking, I put that there because I like this idea and it kinda links to options)
* Are **Options really the way to go ?** What options try to solve is how we can go forward in time many time steps at once, with a predefined behavior. They achieve this simply by using an auxiliary, controlling policy for a short time; enabling the meta agent to consider the environment on a faster time scale. However, it feels a bit more intuitive to talk about sub-goal states; however we might reach them..? This is fundamentally the same thing, but the options don't intrinsically carry information about what state I should end up in; they are just a temporary policy - although you just set gamma(S) = 1_{S=S_target} to reach this result, I guess. Anyway this was motivated by the observation that when humans plan ahead, they do not plan over their action space directly, they just plan in approximate states they will end up in - "I'm going to take the elevator, and go to the bakery to buy a sandwich" - but in the end we are able to plan in future states space because we know what our abilities are, hence what our policy ultimately will yield. Guess it is hard (and not necessarily good practice) to compare with get inspired by human ways of functioning, since we barely know how our brain computes all of this internally; we just experience the illusion of consciousness and choice that arises from the process. This process occurs at a different scale than atomic actions because the brain disregarded most details to make it easier to process and choose from, but this is still our consciousness' observation of the partially observable thoughts of the brain. Hence, this whole thread probably leads nowhere; but dead ends are ends!
* Can we somehow **boost the motivation for learning** by giving intrinsic reward when the agent learns something? So the agent would become knowledge-greedy and start striving for new things. Very, very hard to formalize, but the idea surely yields promising results and even interesting Meta-Learning properties. In a relational setting, this could be implemented as giving intrinsic reward when new connections are made between objects, or when such connections are confirmed by experience. The problem is of credit assignment, in rewarding the *learning process* as opposed to the learnt target. This also comes down to knowledge representation: what is *learning*? We could simply give reward to whatever actions lead to TD errors; but this would favor bad actions in punishment settings.
* If **not Deep Learning, then what?** Looking for an alternative solution to deep; however they offer such interesting learning properties of being non-linear and universal approximators. However, they come with lots of problems like catastrophic inference, slow learning with astronomic amounts of data - and therefore not being fit for online training.
* In a HRL setting, what if one of the **options is simply "explore"**? For example the meta-controller gives its orders and one of them is simply to explore the environment; the agent then acts w.r.t the curious intrinsic reward. Other orders lead to usual reward distributions e.g. one-hot goal-achievement. The advantage of that is then that we have developed an auxiliary task that the controller will learn to master, and in the process, learn much about the environment's dynamics, therefore being stronger when needing to solve other tasks. This kills two birds with one stone (let's say instead, feeds two bird with one seed) because we have a curious algorithm and policy on demand, but we also learn about the general task through auxiliary learning. Ideally, this would blend smoothly in the rest of the meta-slave vocabulary and language, otherwise we'd have to design a special keyword (meta-output -> controller-input) to ask for exploration. This comes back to the question of the meta/slave language, raising the point that just giving (latent) states as goals is not enough to sum up everything the meta could like to convey to the slave.
