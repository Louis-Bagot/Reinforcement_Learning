# Core Ideas (goals) for Reinforcement Learning agents
The first sections just regroup things I want to work on, or think are important for RL.
They obviously do not contain solutions.

## 1. Unsupervised Behavior Learning
One of the most fundamental ideas I want to get through to my agent is the ability to learn behaviors without extrinsic reward.
This is simply motivated by observation of human behavior (and a talk by LeCun): most of what we learn is Unsupervised. We figure out relationships, dynamics, and generalize over the environment, without any reward. Thanks to this, we can then act in the environment with varying goals - or even slightly varying dynamics -, efficiently. For example, in a motor task, we don't need to see the reward at the end to understand that we can move our arm around, reach out for objects, the possible interactions with them. This enables our policy search space to be much more consistent with the environment. Obviously, this only makes sense if the task requested *does* align logically within the environment dynamics (not a random string of actions with no particular meaning or whatnot), but this is actually a fair assumption to carve into the agent's "thought" process.
More formally? From reward-less interactions with the environment, the agent can learn fragment policies (as in, short-term action sequences; or HL controllers) to reach states or situations of *interest*, which can be defined in multiple ways. These include: states that are different from one another (again, defining the distance in many ways), or states in which our actions undoubtedly have an understandable result on the environment (certainty about the state-action dynamics). All in all, the agent can understand its impact on the environment. From this set of fragment policies (it sounds at *lot* like HL), the agent can then explore a more *consistent* set of meta-policies. Ideally, the agent would be able to understand their outcome to enable planning in meta-space, with a relational component to make the planning much more consistent again.
(I'm not too satisfied with this formalization.)
Since it seems so close, let's express this in terms of HL. I want the learning of *options* to be made automatic, independent of the extrinsic reward. This seems to call for Hindsight; but feels like it reaches beyond that.
Another formulation is: we don't want to directly try to solve some goal (reward-based policy search), but rather explore, through a bit of consistent *curiosity*, the state-action/policy space independently, and efficiently learn environment dynamics and policies from this experience.

How can we achieve this, then? (more practically)
### Hindsight
From what I understood, hindsight is the idea that you can learn the behaviors you just realized involuntarily. Learn that whatever sequence of moves you just did led to the outcome you reached; which means a bit learning a model of the environment, but also and beyond that, learning a policy that can achieve the observed behavior.

### Unsupervised Learning
The core idea is simply *generalization*. Situations, states, histories that are similar in ways, and therefore should result in similar outputs. The process by which we want to do this generalization is the interesting part: Unsupervised. Simply classify (or build a distance measure) various properties of our MDP to solve a much lower-level dimensional problem. Informally, we want to build some kind of understanding of the MDP (perhaps without reward), especially our impact on the dynamics, from experience.
Formulated differently, we want to rid ourselves of the environment goal; first developing (from curiosity or whatever) a useful set of policies, and *then* applying them to whatever task we're asked - the idea that if we just change the reward function (but not the whole MDP), much of our knowledge still applies.

* Can we learn some kind of **classification over policies**; linking policies to each other based on their outcome? This would enable to reduce the policy search space by having an idea of policy proximity and different ways to achieve a same goal. More clearly: there's tons of ways to move your arm around, nearly similarly, to achieve the same thing. Such high-dimensional problems, both from an input and output point of view, can often be dramatically be reduced to same *ideas* by generalization, and the understanding that the overall concept is the same. A whole neighborhood of fragment policies can achieve the same result; if Unsupervised Learning can hint on that with some kind of distance measure over policies, the data-efficiency should be dramatically improved.
* **Unsupervised classification of states** (or ideally, latent spaces): perform similar behaviors (atomic/primitive actions) in similar states. This could allow for planning in state-space more efficiently and enable uncertainty to be dealt with in a systematic procedure. This could also help curiosity (next section) by hinting that this state has 'pretty much' already been experienced.

### Artificial Curiosity
When the agent knows nothing of the environment, what should it do? Epsilon-greedy is the simplest (and most frustrating) idea, just randomly trying actions and hoping to get some extrinsic reward from it, to start hunting for it.
How can we beat it?
We want a more consistent approach to *exploration*.
The simplest idea is to strive for the **discovery of new, unseen states** (-action pairs). `TOREAD` all the papers about that \o/.

*Intuitive ideas:*
* motivate the search by **kicking the agent out of known territory** (negative reward for known states or dynamics). Should we record all the trajectories? Unsupervised Learning can help there, to generalize unseen states to similar ones from experience.
* Another idea is **surprise**: model-based. Motivate the search for new states by giving intrinsic reward for mistakes in the model.
* Curiosity in **policy space**: motivate new policies by giving reward for different policies, unseen trajectories. Once again Unsupervised Learning might be able to help there to classify what is different from what.

## 2. Model-based agents and planning
Why?
* The planning - or simply *predicting* (short-horizon planning?) ability of humans is hard to even quantify as it is so fundamental; we predict all the time. In our interactions with each other, or with the environment, we always have some kind of **expectation** of what is going to happen or what might happen (gestures, sentences; physics) which enables us to prepare for the different responses to the anticipated possible outcomes. This severely restrains the policy search space (where a policy can simply be a thought: e.g. anticipate the conclusion of a sentence or sequence thereof, so we can build up our understanding of it before finishing it -- "seeing where this is going" ?)
* Planning allows for much more efficient decision-making: simply **selecting the best action over a set of possible policies** and outcomes. This is a difference with just state-action function: when taking an action, we also consider the policy that fits with this choice (turning left means I'm probably not going to turn right back because my policy wanted me to go right in the first place): we compare policies in a state, not just actions with a single policy form thereon. (could be linked to strategical "plans" in chess, corresponding to completely different trajectories).
* Ideally, we would want to plan in **latent space** and **meta-action (option) space**. They are very linked. When I plan my day, I don't plan every single muscle move. I plan in a more abstract space: going to eat to this or this place, then going to see friends or exercise. We cna see latent space (lower dimensional information than the actual sensory input at the future time), and meta-actions ("going there" implies a very complex sequence of actions). There is a third element coming up there: **simplified state space**. "Going to see friends" implies in a place, at a time, with different possible sets of friends. The latent-space would be the simple idea of all of this at once: not exactly which bar, or where this bottle is; but just the essential information. Knowing which friend is there *is* important in latent space because latent space simply reduces dimensionality (the idea of the friend; not their daily haircut and clothes and everything). The idea of "simplified" state space is then to abstract the latent space once more to give general concepts - this could be seen as another bird's eye view on the same situation; simply smoothing out the space. There are multiple layers of detail for a single situation, which can be seen as abstractions. Instead of creating an entire different idea with "simplified state space", we can rather see it as degrees of confidence in the state approximation (granularity). The granularity isn't necessarily decreasing over time; when planning we might need general ideas about some things, but very precise instructions on particular parts of the future. Therefore it could be interesting to simply use some kind of granularity variable \g_t that gives an idea of how precise we want our model to be for this planned trajectory. It could be defined as a measure over latent states; where we "don't mind if we are \g_t or less far away from the goal state" (distribution over friends and bars we might end up with and in). This distance measure needs to be quite precise then, as to what is and is not important. \g_t could apply to different components of the latent space. This can maybe be linked to the idea of *attention*; not sure if the definition is close in any way: `TOREAD` paper Attention.
