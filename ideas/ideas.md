# Actual true ideas
The thoughts that are a bit more concrete (and hopefully interesting) than the last section of `core.md`.

## World Model Dream Sampling
With Kevin. It seems in the WM paper they use initial game frames to start the dream, when learning full dreamy. However, according to Kevin, a VAE should be able to sample initial dream data only by sampling from the compressed information vector. If this is true, the dream could start anywhere in the world (not only initial distribution), therefore simulating *random starts*. Not only can we rid now ourselves completely of the real-world data, we actually might benefit from it! (with unusual data, preventing overfitting and -a bit- correlated data). It (still) can be argued that random starts might not be a very essential idea since we might sample useless states, that would never show up in the optimal policy. However, if the VAE is trained along with the agent, it should compress mainly experienced data, therefore mostly useful data; hence not completely "random starts" (as it doesn't correspond to any state in the environment), but *semi-random starts* with more weight to interesting states.

## Exploration Option
In a HRL setting, the meta aims to learn options that the worker learns to solve. How options are expressed (i.e. the "language" between the sub-agents) is very task, environment or algo-specific.
Given this goal as input, the worker tries to achieve it following an intrinsic reward (e.g. one-hot encoded "achieved the goal"). It is not easy to bypass the sparse setting in this case as it is the most natural way to reward goals.
However, if we create an additional special option that we will refer to as **explore option** (regardless of how it is encoded language-wise), the corresponding reward could be any exploration/curiosity-encouraging algorithm (prediction, state visits, etc...) - resulting in a *systematic, intelligent exploration policy* and option. The meta-controller would then have an "explore" lever to pull whenever it doesn't know what to do, or it thinks exploration is the way to go (beating schedule methods like annealing epsilon); and this exploration would actually be intelligent (at least make sense, unlike eps_greedy), in the environment. From the worker's perspective, this is an auxiliary task: the weights are trained to learn useful exploration policies, which (in sensible environments) will result in miscellaneous complex behaviors. This would help solve other tasks, as the agent has already learnt something about the environment itself before a goal is even given. Moreover, this could boost hindsight learning, as the policies learnt through systematic and curiosity-driven exploration would be much more original and interesting than eps_greedy policies (reaching unseen states; action sequences resulting in complex surprising consequences) - so much more material could be learnt from the hindsight point of view.
This still comes with the questions "how to convey goals"? If states, it is straightforward but rarely efficient; and the "explore" order isn't very natural then.
Note that if goals are *latent spaces from a shared data-compressor*, we could also add a *radius* information to the goal (resulting in (latent_space, radius) tuple goals) and the reward could be one-hot of ||latent_space - current space|| < radius. This would allow the meta to give an idea of **approximate goal**, as in "reach this overall state" without needing to be perfectly precise; it could get as precise as it wishes.
