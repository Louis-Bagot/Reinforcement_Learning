# Exploration Option, additional head or policy...
... Trying to dig into that.
## General Idea & advantages
We want to be able to train part of the agent on intrinsic reward to boost exploration. How this could be done precisely is still to be worked on, and will be specified in the following subsections. In the following, *Explorer* refers to the subcomponent (policy) of the agent that tries to explore (most likely in a HRL kind of setting).

So, writing down all **possible advantages** of this idea:
* Most obviously, the agent now has access to **an "exploration" lever** it can pull in any situation. It can therefore learn to use it in situations of *doubt* or simply when exploration may be the best policy.
* This is a much stronger exploration scheme than epsilon greedy, as it is a systematic, intelligent exploration (not random, e.g. eps greedy or adding noise in actions).
* **Off-policy learning** of the true reward (or sub-goal rewards) when the Explorer is acting; Off-policy learning of the exploration policy when the Explorer is not acting. So, all components always learn something.
* **Easy to plug** in any agent
* **Low overhead**: at worst twice as long; but weight sharing enables to trim the biggest parts of the computation (visual parts)
* If the components share part of their architecture (e.g. vision module, or the first FC layers - in a Dueling architecture kind of setting), then this becomes **auxiliary task learning**: the shared part of the environment is trained both for exploration and exploitation, leading to potential faster training and better generalization.
* In **model-based learning**, the Explorer could kick us out of known territory. With some measure of *how confident our model is*, we could export the exploratory behaviors in the real environment so that the model learns new parts of the environment.
* Solves noisy-TV ? Since the Exploiter doesn't give a damn about it; as soon as it takes back control, it will lead us away

Now focusing on the **shortcomings** of the idea:
* Within limitations of the exploration scheme covered by the intrinsic reward: 1/ Mainly, *can it cover consistently the state-action space*? 2/ Will it come back to states from long ago? (which might be desired, especially with a stronger policy).
* As a followup on the previous point: having to choose a single motivation/curiosity/exploration scheme. It is often environment-dependent (vision-based -> surprise, gridworlds -> state visits..). Ideally, we would want the agent to *learn the exploration scheme*. Could we somehow have a part of the network that gives out intrinsic reward? What would that be motivated on? That's basically the same problem as learning options.
* To ensure exploration, we have to keep hitting that lever. How can we ensure that? Obviously, through eps-soft policies, but this is a bit limiting. In addition...
* eps-greedy choosing the exploration action may be **detrimental** to our agent's performance: if the agent is constrained to perform the explore action when on an optimal trajectory, it will be kicked off of it and will have to get back on track. This is quite good for generalization and the diversity of policies the agent masters, however the performance is sure to drop compared to the optimal trajectory. Obviously, lowering epsilon in testing is an option, but will still hinder the agent when learning the optimal trajectory. Another option would be to find an efficient way to let the Meta stop the Explorer and take back control.
* How do we **stop**? `TOREAD`: Papers by Precup on how to terminate an option. Ideas could be: the Meta chooses when to stop the Explorer; or the Explorer decides to. In the first option, the Meta would then need to be queried each time. Would the termination decision be an action? How should we consider this theoretically, and train it? In the second case, the Explorer now has a *stop* action. It could learn to trigger it when exploration become uninteresting, e.g. no more reward? But then it would quickly get bored of acting at all; or act forever (as there could always be something to explore! e.g. Minecraft or such open worlds).

## HRL setting with goals
In a HRL setting, the meta aims to learn options that the worker learns to solve. How options are expressed (i.e. the "language" between the sub-agents) is very task, environment or algo-specific.
Given this goal as input, the worker tries to achieve it following an intrinsic reward (e.g. one-hot encoded "achieved the goal"). It is not easy to bypass the sparse setting in this case as it is the most natural way to reward goals.
However, if we create an additional special option that we will refer to as **explore option** (regardless of how it is encoded language-wise), the corresponding reward could be any exploration/curiosity-encouraging algorithm (prediction, state visits, etc...) - resulting in a *systematic, intelligent exploration policy* and option. The meta-controller would then have an "explore" lever to pull whenever it doesn't know what to do, or it thinks exploration is the way to go (beating schedule methods like annealing epsilon); and this exploration would actually be intelligent (at least make sense, unlike eps_greedy), in the environment. From the worker's perspective, this is an auxiliary task: the weights are trained to learn useful exploration policies, which (in sensible environments) will result in miscellaneous complex behaviors. This would help solve other tasks, as the agent has already learnt something about the environment itself before a goal is even given. Moreover, this could boost hindsight learning, as the policies learnt through systematic and curiosity-driven exploration would be much more original and interesting than eps_greedy policies (reaching unseen states; action sequences resulting in complex surprising consequences) - so much more material could be learnt from the hindsight point of view.
This still comes with the questions "how to convey goals"? If states, it is straightforward but rarely efficient; and the "explore" order isn't very natural then.
Note that if goals are *latent spaces from a shared data-compressor*, we could also add a *radius* information to the goal (resulting in (latent_space, radius) tuple goals) and the reward could be one-hot of ||latent_space - current space|| < radius. This would allow the meta to give an idea of **approximate goal**, as in "reach this overall state" without needing to be perfectly precise; it could get as precise as it wishes.

## Explorer policy
**Simplification**: The controller only exists for exploration, so we don't have to convey goals. The explore option triggers said controller we might call Explorer, which is trained on some intrinsic reward scheme for exploration.
The Explorer can learn an exploratory GVF off-policy from the Meta's experience (how to explore, e.g. which frames we'll see - and don't want to see again -, or states we've already been to), while the Meta can learn the true VF off-policy from the Explorer's behavior.

Note that it could be possible that the explorer policy is *not actually learnt* but just some kind of systematic state Explorer (i.e. non parametric policy kind of things). The rest could still learn off-policy, but this can be a good insight into the possibilities. Let's not limit ourselves to just NNs when they might not be too relevant.

## Additional
for the Meta to know when to use exploration, the state alone might not be enough because it doesn't contain the information that it *doesn't know about it* and therefore that Exploring might be a good idea: it only sees the state, has no idea that this state is "new" in any way, so there is no reason it would choose the Explore option rather than any action. An RL agent, even with function approximation, tries to reproduces past experiences, so the information about the "new-ness" is not carried without memory. If that makes sense.
Anyway, so how can we circumvent that?
An idea might be to use the *intrinsic reward as input to the Meta* {or just the prediction error, if we're doing curiosity-based exploration}. The idea is that when this value is high, the Explorer is likely to perform well - or at least better than the agent alone, because the state is new.
**Weight sharing with the prediction network** from the Curious reward, also?
That's an auxiliary task! Would only make sense for features that make sense, e.g. IDF or VAE, but not RF.
If we try to combine it with DIAYN, every time the Explore Option is called, we could sample one of the policy/skills. The Exploiter (Meta) could still learn Off-policy directly from those skills. Or we could add skills as options in addition to atomic actions but that kinda loses the core point and becomes HRL again.
We can always add Maximum Entropy optimization to our Explorer (in addition to other rewards) to make sure it stays random and doesn't specialize too much. Adds a hyperparameter though.
